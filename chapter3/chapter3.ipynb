{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter3 Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Linear model의 기본 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = w_1x_1 + w_2x_2 + \\ldots + w_dx_d + b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\mathbf{w}^T\\mathbf{x} + b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{w} = (w_1; w_2; \\ldots; w_d)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 모델 자체는 형식이 매우 간단하고 만들기 쉬우나, 머신러닝의 중요한 기본 사상을 담고 있습니다. <br>\n",
    "강력한 성능을 가진 non-linear model들은 linear model을 기반으로 하여 layer을 쌓아 올리거나(ex. DNN) 고차원으로 투영(ex. SVM)하여 만들어집니다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/dnn.png) <br>\n",
    "※ DNN\n",
    "\n",
    "![](img/svm.png) <br>\n",
    "※ SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 회귀는 최대한 정확하게 실제 데이터를 예측하는 선형 모델을 학습하는 것을 목표로 합니다. <br>\n",
    "속성이 하나 뿐인 가장 간단한 상황을 고려했을 때, 선형 회귀는 다음과 같은 함수를 학습합니다. <br>\n",
    "$$\n",
    "f(x_i) = w x_i + b \\text{을 통해} f(x_i) \\approx y_i \\text{를 얻는다.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 식에서 $w$와 $b$는 $f(x)$와 $y$의 **차이**를 어떻게 측정하느냐에 달렸습니다. <br>\n",
    "이 **차이**는 일반적으로 MSE(Mean Squared Error)를 사용하여 구하게 됩니다.\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "(w^*, b^*) = \\underset{(w,b)}{\\mathrm{arg\\,min}} \\sum_{i=1}^m (f(x_i) - y_i)^2 \\\\\n",
    "= \\underset{(w,b)}{\\mathrm{arg\\,min}} \\sum_{i=1}^m (y_i - wx_i - b)^2.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE는 기하학적인 의미가 있습니다. 유클리드 거리(Euclidean distance)를 사용하기 때문입니다. <br><br>\n",
    "<img src=\"img/Euclidean_Distance.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE를 최소화 하는 방법으로 모델의 해를 구하는 방법을 최소제곱법(Least Square Method)이라 부르며, <br>\n",
    "선형 회귀에서 최소제곱법은 데이터 포인트와의 유클리드 거리의 합이 가장 작은 하나의 직선을 찾는 것을 목표로 합니다. <br><br>\n",
    "<img src=\"img/linear_regression.png\" width=\"650\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w$와 $b$의 해를 찾는 것은 convex function인 $E_{(w,b)} = \\sum_{i=1}^m (y_i - wx_i - b)^2$를 최소화하는 과정이고, <br>\n",
    "이를 선형 회귀 모델의 최소제곱 파라미터 예측(parameter estimation)이라 부릅니다. <br>\n",
    "우리는 $E_{(w,b)}$로 $w$와 $b$에 대해 다음과 같은 식을 얻을 수 있습니다.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E_{(w,b)}}{\\partial w} &= 2 \\left( w \\sum_{i=1}^m x_i^2 - \\sum_{i=1}^m (y_i - b) x_i \\right), \\\\\n",
    "\\frac{\\partial E_{(w,b)}}{\\partial b} &= 2 \\left( mb - \\sum_{i=1}^m (y_i - wx_i) \\right),\n",
    "\\end{align*}\n",
    "\n",
    "그리고 위 두 식을 0으로 만들어 $w$와 $b$ 최적해(optimum solution)의 닫힌 해(closed-form solution)를 구할 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "w &= \\frac{\\sum_{i=1}^m y_i (x_i - \\bar{x})}{\\sum_{i=1}^m x_i^2 - \\frac{1}{m} \\left( \\sum_{i=1}^m x_i \\right)^2}, \\\\\n",
    "b &= \\frac{1}{m} \\sum_{i=1}^m (y_i - wx_i),\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 모델은 매우 간단하지만 변화무쌍합니다. <br>\n",
    "다음과 같은 선형 회귀 모델이 있다고 가정합니다. <br>\n",
    "$$\n",
    "y = w^Tx + b\n",
    "$$\n",
    "이 때, 데이터 포인트(input)에 대응하는 결괏값(output)이 exponential scale로 변화한다고 가정하면 다음과 같은 선형 모델이 만들어질 수 있습니다.\n",
    "$$\n",
    "\\ln(y) = w^Tx + b\n",
    "$$\n",
    "이것이 바로 로그 선형 회귀(log-linear regression)이며, 실제로는 $e^{w^Tx+b}$를 y에 근사시키는 것입니다. <br>\n",
    "형식적으로는 선형 회귀이나 실질적으로는 비선형 함수의 투영입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/log-linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 일반적으로는 단조 미분 가능 함수 $g(\\cdot)$를 고려하여 다음과 같은 식으로 만듭니다.\n",
    "$$\n",
    "y = g^{-1}(w^Tx + b)\n",
    "$$\n",
    "이렇게 얻은 모델은 일반화 선형 모델(generalize linear model)이라 부르고, 함수 $g(\\cdot)$는 링크 함수(link function)라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀가 아닌 분류 문제일때도 단조 미분 가능 함수 $g(\\cdot)$ 하나만 찾아내어 분류 문제의 실제 레이블 y와 선형 회귀 모델의 예측값을 연결해 주기만 하면 됩니다. <br>\n",
    "결괏값이 $y \\in {0,1}$인 이진 분류 문제일 때, 선형 회귀 모델 $z = w^T + b$의 값은 연속적인 실수이므로 0 또는 1로 변환해야 합니다. <br>\n",
    "가장 이상적인 방법은 단위 계단 함수(unit-step funciton)를 활용하는 것입니다. <br>\n",
    "\n",
    "$$\n",
    "y = \n",
    "\\begin{cases} \n",
    "0, & \\text{if } z < 0; \\\\\n",
    "0.5, & \\text{if } z = 0; \\\\\n",
    "1, & \\text{if } z > 0,\n",
    "\\end{cases}\n",
    "$$\n",
    "![](img/unit_step_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 위 그림에서 볼 수 있듯이, 단위 계단 함수는 불연속적(미분불가능)이므로 대체 함수를 찾아야 합니다. <br>\n",
    "이 경우 로지스틱 함수(logistic function)이 자주 쓰입니다.\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "로지스틱 함수는 일종의 시그모이드 함수(Sigmoid function)입니다.\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1+e^{-{(w^Tx + b)}}}\n",
    "$$\n",
    "$$\n",
    "\\ln \\frac{y}{1-y} = w^Tx + b\n",
    "$$\n",
    "\n",
    "만약 y를 샘플 x가 양성일 가능성으로 본다면, 1 - y는 반대로 음성일 가능성이 됩니다. <br>\n",
    "이 둘을 비교하는 값을 오즈(odds)라 칭하며, 다음과 같은 형태입니다.\n",
    "\n",
    "$$\n",
    "\\frac{y}{1-y}\n",
    "$$\n",
    "\n",
    "위 식에서 알 수 있는 것은 사실상 선형 회귀 모델의 예측 결과를 사용하여 실제 데이터의 로그 오즈에 근사한다는 것입니다. <br>\n",
    "따라서 이러한 모델을 로지스틱 회귀(logistic regressino) 또는 로짓 회귀(logit regression)라고 부릅니다. <br>\n",
    "여기서 주의해야 할 점은 명칭은 \"회귀\"지만 사실상 \"분류\" 학습법이라는 것입니다. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀에는 많은 장점이 있습니다.\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ※ 딥러닝에서의 logit?\n",
    "[0,1] 범위인 확률을 [−∞,∞]범위로 넓히는 logit의 특성때문에, 딥러닝에서는 확률화되지 않은 날 예측 결과를 logit이라고 칭한다. <br>\n",
    "멀티 클래스 분류문제에서는 보통 softmax 함수의 입력으로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
