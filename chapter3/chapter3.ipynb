{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter3 Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Linear model의 기본 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = w_1x_1 + w_2x_2 + \\ldots + w_dx_d + b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\mathbf{w}^T\\mathbf{x} + b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{w} = (w_1; w_2; \\ldots; w_d)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 모델 자체는 형식이 매우 간단하고 만들기 쉬우나, 머신러닝의 중요한 기본 사상을 담고 있습니다. <br>\n",
    "강력한 성능을 가진 non-linear model들은 linear model을 기반으로 하여 layer을 쌓아 올리거나(ex. DNN) 고차원으로 투영(ex. SVM)하여 만들어집니다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/dnn.png) <br>\n",
    "※ DNN\n",
    "\n",
    "![](img/svm.png) <br>\n",
    "※ SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 회귀는 최대한 정확하게 실제 데이터를 예측하는 선형 모델을 학습하는 것을 목표로 합니다. <br>\n",
    "속성이 하나 뿐인 가장 간단한 상황을 고려했을 때, 선형 회귀는 다음과 같은 함수를 학습합니다. <br>\n",
    "$$\n",
    "f(x_i) = w x_i + b \\text{을 통해} f(x_i) \\approx y_i \\text{를 얻는다.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 식에서 $w$와 $b$는 $f(x)$와 $y$의 **차이**를 어떻게 측정하느냐에 달렸습니다. <br>\n",
    "이 **차이**는 일반적으로 MSE(Mean Squared Error)를 사용하여 구하게 됩니다.\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "(w^*, b^*) = \\underset{(w,b)}{\\mathrm{arg\\,min}} \\sum_{i=1}^m (f(x_i) - y_i)^2 \\\\\n",
    "= \\underset{(w,b)}{\\mathrm{arg\\,min}} \\sum_{i=1}^m (y_i - wx_i - b)^2.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE는 기하학적인 의미가 있습니다. 유클리드 거리(Euclidean distance)를 사용하기 때문입니다. <br><br>\n",
    "<img src=\"img/Euclidean_Distance.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE를 최소화 하는 방법으로 모델의 해를 구하는 방법을 최소제곱법(Least Square Method)이라 부르며, <br>\n",
    "선형 회귀에서 최소제곱법은 데이터 포인트와의 유클리드 거리의 합이 가장 작은 하나의 직선을 찾는 것을 목표로 합니다. <br><br>\n",
    "<img src=\"img/linear_regression.png\" width=\"650\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w$와 $b$의 해를 찾는 것은 convex function인 $E_{(w,b)} = \\sum_{i=1}^m (y_i - wx_i - b)^2$를 최소화하는 과정이고, <br>\n",
    "이를 선형 회귀 모델의 최소제곱 파라미터 예측(parameter estimation)이라 부릅니다. <br>\n",
    "우리는 $E_{(w,b)}$로 $w$와 $b$에 대해 다음과 같은 식을 얻을 수 있습니다.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E_{(w,b)}}{\\partial w} &= 2 \\left( w \\sum_{i=1}^m x_i^2 - \\sum_{i=1}^m (y_i - b) x_i \\right), \\\\\n",
    "\\frac{\\partial E_{(w,b)}}{\\partial b} &= 2 \\left( mb - \\sum_{i=1}^m (y_i - wx_i) \\right),\n",
    "\\end{align*}\n",
    "\n",
    "그리고 위 두 식을 0으로 만들어 $w$와 $b$ 최적해(optimum solution)의 닫힌 해(closed-form solution)를 구할 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "w &= \\frac{\\sum_{i=1}^m y_i (x_i - \\bar{x})}{\\sum_{i=1}^m x_i^2 - \\frac{1}{m} \\left( \\sum_{i=1}^m x_i \\right)^2}, \\\\\n",
    "b &= \\frac{1}{m} \\sum_{i=1}^m (y_i - wx_i),\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 모델은 매우 간단하지만 변화무쌍합니다. <br>\n",
    "다음과 같은 선형 회귀 모델이 있다고 가정합니다. <br>\n",
    "$$\n",
    "y = w^Tx + b\n",
    "$$\n",
    "이 때, 데이터 포인트(input)에 대응하는 결괏값(output)이 exponential scale로 변화한다고 가정하면 다음과 같은 선형 모델이 만들어질 수 있습니다.\n",
    "$$\n",
    "\\ln(y) = w^Tx + b\n",
    "$$\n",
    "이것이 바로 로그 선형 회귀(log-linear regression)이며, 실제로는 $e^{w^Tx+b}$를 y에 근사시키는 것입니다. <br>\n",
    "형식적으로는 선형 회귀이나 실질적으로는 비선형 함수의 투영입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/log-linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 일반적으로는 단조 미분 가능 함수 $g(\\cdot)$를 고려하여 다음과 같은 식으로 만듭니다.\n",
    "$$\n",
    "y = g^{-1}(w^Tx + b)\n",
    "$$\n",
    "이렇게 얻은 모델은 일반화 선형 모델(generalize linear model)이라 부르고, 함수 $g(\\cdot)$는 링크 함수(link function)라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀가 아닌 분류 문제일때도 단조 미분 가능 함수 $g(\\cdot)$ 하나만 찾아내어 분류 문제의 실제 레이블 y와 선형 회귀 모델의 예측값을 연결해 주기만 하면 됩니다. <br>\n",
    "결괏값이 $y \\in {0,1}$인 이진 분류 문제일 때, 선형 회귀 모델 $z = w^T + b$의 값은 연속적인 실수이므로 0 또는 1로 변환해야 합니다. <br>\n",
    "가장 이상적인 방법은 단위 계단 함수(unit-step funciton)를 활용하는 것입니다. <br>\n",
    "\n",
    "$$\n",
    "y = \n",
    "\\begin{cases} \n",
    "0, & \\text{if } z < 0; \\\\\n",
    "0.5, & \\text{if } z = 0; \\\\\n",
    "1, & \\text{if } z > 0,\n",
    "\\end{cases}\n",
    "$$\n",
    "![](img/unit_step_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 위 그림에서 볼 수 있듯이, 단위 계단 함수는 불연속적(미분불가능)이므로 대체 함수를 찾아야 합니다. <br>\n",
    "이 경우 로지스틱 함수(logistic function)이 자주 쓰입니다.\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "로지스틱 함수는 일종의 시그모이드 함수(Sigmoid function)입니다.\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1+e^{-{(w^Tx + b)}}}\n",
    "$$\n",
    "$$\n",
    "\\ln \\frac{y}{1-y} = w^Tx + b\n",
    "$$\n",
    "\n",
    "만약 y를 샘플 x가 양성일 가능성으로 본다면, 1 - y는 반대로 음성일 가능성이 됩니다. <br>\n",
    "이 둘을 비교하는 값을 오즈(odds)라 칭하며, 다음과 같은 형태입니다.\n",
    "\n",
    "$$\n",
    "\\frac{y}{1-y}\n",
    "$$\n",
    "\n",
    "위 식에서 알 수 있는 것은 사실상 선형 회귀 모델의 예측 결과를 사용하여 실제 데이터의 로그 오즈에 근사한다는 것입니다. <br>\n",
    "따라서 이러한 모델을 로지스틱 회귀(logistic regressino) 또는 로짓 회귀(logit regression)라고 부릅니다. <br>\n",
    "여기서 주의해야 할 점은 명칭은 \"회귀\"지만 사실상 \"분류\" 학습법이라는 것입니다. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀에는 많은 장점이 있습니다. <br><br>\n",
    "1. input 데이터의 분포에 대한 가정이 불필요 <br>\n",
    "로지스틱 회귀는 분류 문제를 해결하기 위한 모델을 만들 때, 데이터의 사전 분포에 대한 복잡한 가정을 할 필요가 없습니다. 예를 들어, 선형 회귀분석 같은 다른 통계 모델들은 데이터가 정규 분포를 따른다는 가정을 종종 필요로 합니다. 하지만 로지스틱 회귀는 이러한 가정 없이 데이터로부터 직접적으로 분류 가능성을 모델링합니다. 이는 데이터 분포에 대한 잘못된 가정으로 인해 발생할 수 있는 오류를 줄일 수 있는 장점이 있습니다.\n",
    "\n",
    "2. 상대적인 확률 예측 <br>\n",
    "로지스틱 회귀는 단순히 클래스를 예측하는 것뿐만 아니라, 해당 클래스에 속할 확률을 추정하는 능력이 있습니다. 이는 단순히 '예' 또는 '아니오'로 분류하는 것보다 훨씬 더 많은 정보를 제공합니다. 예를 들어, 의료 진단에서 환자가 특정 질병을 가질 확률을 알아내는 것은 치료 방법을 결정할 때 매우 중요할 수 있습니다. 로지스틱 회귀를 통해 얻은 확률은 이러한 의사결정에 중요한 근거를 제공합니다. 단, 로지스틱 회귀의 결과는 실제 확률이 아닌, 오즈(log odds) 또는 로짓(logit) 값입니다. 이 로짓 값을 확률로 변환하기 위해서는 로지스틱 함수를 사용하여, 다음과 같이 계산할 수 있습니다.\n",
    "$$\n",
    "\\frac{e^{logit}}{1+e^{logit}}\n",
    "$$\n",
    "이렇게 변환된 값은 0과 1 사이의 값으로, 어떤 사건이 발생할 상대적인 확률을 나타내게 됩니다. <br>\n",
    "3. 컨벡스 함수: 로지스틱 회귀가 최적화하려는 목표 함수는 컨벡스 함수입니다. 컨벡스 함수란, 그래프의 임의의 두 점을 연결하는 선분이 항상 함수의 그래프 위에 있을 때 그러한 성질을 가진 함수를 말합니다. 이 성질은 함수가 단 한 개의 최소값(global minimum)만을 가지며, 그 최소값을 찾아가는 길이 항상 존재한다는 것을 의미합니다. 이로 인해, 로지스틱 회귀 모델을 학습시킬 때, 경사 하강법 같은 다양한 수치 최적화 알고리즘을 사용할 수 있고, 이 알고리즘들은 모두 최적해를 찾는 데에 효과적입니다. 이는 로지스틱 회귀가 실용적인 머신러닝 방법론 중 하나로 자리 잡게 하는 중요한 요소입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ※ 딥러닝에서의 logit?\n",
    "[0,1] 범위인 확률을 [−∞,∞]범위로 넓히는 logit의 특성때문에, 딥러닝에서는 확률화되지 않은 날 예측 결과를 logit이라고 칭한다. <br>\n",
    "멀티 클래스 분류문제에서는 보통 softmax 함수의 입력으로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
